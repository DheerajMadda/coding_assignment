{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3d8ed8",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da6e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c8a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from thop import profile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import YoloV8I, YoloV8I_CONFIGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9846990",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b9b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module):\n",
    "    \"\"\"\n",
    "    This method computes the model size\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "    model : object\n",
    "        A torch.nn.Module object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "\n",
    "    \"\"\"\n",
    "    KILOBYTE_TO_BYTE = MEGABYTE_TO_KILOBYTE = 1024\n",
    "    \n",
    "    size_of_parameters = sum([param.element_size() * param.nelement() for param in model.parameters()])\n",
    "    size_of_buffers = sum([buf.element_size() * buf.nelement() for buf in model.buffers()])\n",
    "    model_size = size_of_parameters + size_of_buffers # Bytes\n",
    "    model_size_mb = model_size / (KILOBYTE_TO_BYTE * MEGABYTE_TO_KILOBYTE) # MegaBytes\n",
    "    return round(model_size_mb, 3)\n",
    "\n",
    "def get_inference_time(\n",
    "    model: nn.Module,\n",
    "    inputs: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    n_iters:int=10,\n",
    "    gpu_warmup:bool=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This method computes the inference time of the model for both targets, cpu and cuda\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "    model : object\n",
    "        A torch.nn.Module object\n",
    "    inputs : object\n",
    "        A numpy array or torch tensor\n",
    "    device : [object, str]\n",
    "        A string or torch.device object representing the device. E.g \"cpu\" or torch.device(\"cpu\")\n",
    "    n_iters : int\n",
    "        An integer value specifying the number of iterations to be performed for a given \n",
    "        input batch\n",
    "    gpu_warmup : bool\n",
    "        If set to True; warms up the GPU before computing the actual GPU inference time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    is_gpu = False if str(device) == \"cpu\" else True\n",
    "    batch_size = inputs[0].shape[0]\n",
    "    model.to(device).eval()\n",
    "    timings = np.zeros((n_iters, 1))\n",
    "\n",
    "    if is_gpu:\n",
    "        if gpu_warmup:\n",
    "            # GPU-WARM-UP\n",
    "            warm_ups = [\n",
    "                [(1, 4), 12],\n",
    "                [(5, 8), 9],\n",
    "                [(8, 12), 6],\n",
    "                [(12, 16), 3]\n",
    "            ]\n",
    "\n",
    "            if batch_size > 16:\n",
    "                num_warm_ups = 1\n",
    "            else:\n",
    "                for warm_up in warm_ups:\n",
    "                    if warm_up[0][0] <= batch_size <= warm_up[0][1]:\n",
    "                        num_warm_ups = warm_up[1]\n",
    "                        break\n",
    "\n",
    "            print(f\"Performing GPU warm up with {num_warm_ups} iterations...\")\n",
    "            for _ in range(num_warm_ups):\n",
    "                model(*inputs)\n",
    "            print(\"GPU warmup complete!\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for rep in range(n_iters):\n",
    "                starter = torch.cuda.Event(enable_timing=True)\n",
    "                ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "                starter.record()\n",
    "                model(*inputs)\n",
    "                ender.record()\n",
    "\n",
    "                # WAIT FOR GPU SYNC\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                elapsed_time = starter.elapsed_time(ender)  # milliseconds\n",
    "                timings[rep] = elapsed_time\n",
    "\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            for rep in range(n_iters):\n",
    "                start_time = time.perf_counter()\n",
    "                model(*inputs)\n",
    "                end_time = time.perf_counter()\n",
    "                elapsed_time = (end_time - start_time) / 1000  # milliseconds\n",
    "                timings[rep] = elapsed_time\n",
    "\n",
    "    avg_batch_inference_time = round(np.sum(timings)/ n_iters, 3)\n",
    "    avg_sample_inference_time = round(avg_batch_inference_time / batch_size, 3)\n",
    "\n",
    "    return avg_batch_inference_time, avg_sample_inference_time\n",
    "\n",
    "def get_model_metrics(\n",
    "    model: nn.Module,\n",
    "    inputs: torch.Tensor,\n",
    "    device: torch.device\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This method computes the model metrics - MACs, FLOPs and number of parameters\n",
    "\n",
    "        Args\n",
    "        ----------\n",
    "        model : object\n",
    "            A torch.nn.Module object\n",
    "        inputs : object\n",
    "            A numpy array or torch tensor\n",
    "        device : object | str\n",
    "            A string or torch.device object representing the device. E.g \"cpu\" or torch.device(\"cpu\")\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = inputs[0].shape[0]\n",
    "        model.to(device).eval()\n",
    "        \n",
    "        MACs, params = profile(model, inputs=inputs, verbose=False)\n",
    "\n",
    "        M_params = round(params * 1e-6, 6)\n",
    "        M_MACs_batch = round(MACs * 1e-9, 6) \n",
    "        M_FLOPs_batch = 2 * M_MACs_batch\n",
    "\n",
    "        M_MACs = M_MACs_batch / batch_size\n",
    "        M_FLOPs = M_FLOPs_batch / batch_size\n",
    "\n",
    "        return M_MACs, M_MACs_batch, M_FLOPs, M_FLOPs_batch, M_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bc53a",
   "metadata": {},
   "source": [
    "## 3. Define Model, device and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2120c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 80\n",
    "\n",
    "model_type = \"m\"\n",
    "model_config = YoloV8I_CONFIGS[model_type]\n",
    "model_config.num_classes = NUM_CLASSES\n",
    "\n",
    "model = YoloV8I(model_config)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "inputs = [torch.randn(16, 3, 480, 640).to(device)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9ef91",
   "metadata": {},
   "source": [
    "## 4. Perform Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42274a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size in MB: 77.594\n"
     ]
    }
   ],
   "source": [
    "# get model size\n",
    "model_size = get_model_size(model)\n",
    "print(f\"Model Size in MB: {model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735e69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs per sample (10^9): 24.3368815\n",
      "FLOPs per sample (10^9): 48.673763\n",
      "MACs per batch (10^9): 389.390104\n",
      "FLOPs per batch (10^9): 778.780208\n",
      "Number of Parameters (Million): 20.280884\n"
     ]
    }
   ],
   "source": [
    "M_MACs, M_MACs_batch, M_FLOPs, M_FLOPs_batch, M_params = get_model_metrics(model, inputs, device)\n",
    "print(f\"MACs per sample (10^9): {M_MACs}\")\n",
    "print(f\"FLOPs per sample (10^9): {M_FLOPs}\")\n",
    "print(f\"MACs per batch (10^9): {M_MACs_batch}\")\n",
    "print(f\"FLOPs per batch (10^9): {M_FLOPs_batch}\")\n",
    "print(f\"Number of Parameters (Million): {M_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c245bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GPU warm up with 3 iterations...\n",
      "GPU warmup complete!\n",
      "Sample Inference Time (ms): 5.876\n",
      "Batch Inference Time (ms): 94.009\n"
     ]
    }
   ],
   "source": [
    "avg_batch_inference_time, avg_sample_inference_time = get_inference_time(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    device=device,\n",
    "    n_iters=10,\n",
    "    gpu_warmup=True\n",
    ")\n",
    "print(f\"Sample Inference Time (ms): {avg_sample_inference_time}\")\n",
    "print(f\"Batch Inference Time (ms): {avg_batch_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c48c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame({\n",
    "    \"Model\" : [\"YOLOv8I-m\"],\n",
    "    \"Model_size (MB)\" : [model_size],\n",
    "    \"Num_Parameters (Million)\": [M_params],\n",
    "    \"Sample MACs (10^9)\" : [M_MACs],\n",
    "    \"Sample FLOPs (10^9)\" : [M_FLOPs],\n",
    "    \"Batch MACs (10^9)\" : [M_MACs_batch],\n",
    "    \"Batch FLOPs (10^9)\" : [M_FLOPs_batch],\n",
    "    \"Sample Latency (ms)\": [avg_sample_inference_time],\n",
    "    \"Batch Latency (ms)\": [avg_batch_inference_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f34a8",
   "metadata": {},
   "source": [
    "## 5. Compare with Standard YOLOv8 and YOLOv12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22e0ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\dheer\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3ab6f",
   "metadata": {},
   "source": [
    "### YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72967e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov8 = YOLO(\"yolov8m.pt\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22898119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size in MB: 98.938\n"
     ]
    }
   ],
   "source": [
    "# get model size\n",
    "model_size = get_model_size(yolov8)\n",
    "print(f\"Model Size in MB: {model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c46f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs per sample (10^9): 29.745158375\n",
      "FLOPs per sample (10^9): 59.49031675\n",
      "MACs per batch (10^9): 475.922534\n",
      "FLOPs per batch (10^9): 951.845068\n",
      "Number of Parameters (Million): 25.90264\n"
     ]
    }
   ],
   "source": [
    "M_MACs, M_MACs_batch, M_FLOPs, M_FLOPs_batch, M_params = get_model_metrics(yolov8, inputs, device)\n",
    "print(f\"MACs per sample (10^9): {M_MACs}\")\n",
    "print(f\"FLOPs per sample (10^9): {M_FLOPs}\")\n",
    "print(f\"MACs per batch (10^9): {M_MACs_batch}\")\n",
    "print(f\"FLOPs per batch (10^9): {M_FLOPs_batch}\")\n",
    "print(f\"Number of Parameters (Million): {M_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8a5a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GPU warm up with 3 iterations...\n",
      "GPU warmup complete!\n",
      "Sample Inference Time (ms): 6.425\n",
      "Batch Inference Time (ms): 102.805\n"
     ]
    }
   ],
   "source": [
    "avg_batch_inference_time, avg_sample_inference_time = get_inference_time(\n",
    "    model=yolov8,\n",
    "    inputs=inputs,\n",
    "    device=device,\n",
    "    n_iters=10,\n",
    "    gpu_warmup=True\n",
    ")\n",
    "print(f\"Sample Inference Time (ms): {avg_sample_inference_time}\")\n",
    "print(f\"Batch Inference Time (ms): {avg_batch_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed97d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov8_df = pd.DataFrame({\n",
    "    \"Model\" : [\"YOLOv8-m\"],\n",
    "    \"Model_size (MB)\" : [model_size],\n",
    "    \"Num_Parameters (Million)\": [M_params],\n",
    "    \"Sample MACs (10^9)\" : [M_MACs],\n",
    "    \"Sample FLOPs (10^9)\" : [M_FLOPs],\n",
    "    \"Batch MACs (10^9)\" : [M_MACs_batch],\n",
    "    \"Batch FLOPs (10^9)\" : [M_FLOPs_batch],\n",
    "    \"Sample Latency (ms)\": [avg_sample_inference_time],\n",
    "    \"Batch Latency (ms)\": [avg_batch_inference_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf101f5",
   "metadata": {},
   "source": [
    "### YOLOv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f976494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12m.pt to 'yolo12m.pt': 100% ━━━━━━━━━━━━ 39.0/39.0MB 31.0MB/s 1.3s0.0s\n"
     ]
    }
   ],
   "source": [
    "yolov12 = YOLO(\"yolo12m.pt\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab508ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size in MB: 77.311\n"
     ]
    }
   ],
   "source": [
    "# get model size\n",
    "model_size = get_model_size(yolov12)\n",
    "print(f\"Model Size in MB: {model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11a0b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs per sample (10^9): 25.529721625\n",
      "FLOPs per sample (10^9): 51.05944325\n",
      "MACs per batch (10^9): 408.475546\n",
      "FLOPs per batch (10^9): 816.951092\n",
      "Number of Parameters (Million): 20.201216\n"
     ]
    }
   ],
   "source": [
    "M_MACs, M_MACs_batch, M_FLOPs, M_FLOPs_batch, M_params = get_model_metrics(yolov12, inputs, device)\n",
    "print(f\"MACs per sample (10^9): {M_MACs}\")\n",
    "print(f\"FLOPs per sample (10^9): {M_FLOPs}\")\n",
    "print(f\"MACs per batch (10^9): {M_MACs_batch}\")\n",
    "print(f\"FLOPs per batch (10^9): {M_FLOPs_batch}\")\n",
    "print(f\"Number of Parameters (Million): {M_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28b0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GPU warm up with 3 iterations...\n",
      "GPU warmup complete!\n",
      "Sample Inference Time (ms): 8.345\n",
      "Batch Inference Time (ms): 133.523\n"
     ]
    }
   ],
   "source": [
    "avg_batch_inference_time, avg_sample_inference_time = get_inference_time(\n",
    "    model=yolov12,\n",
    "    inputs=inputs,\n",
    "    device=device,\n",
    "    n_iters=10,\n",
    "    gpu_warmup=True\n",
    ")\n",
    "print(f\"Sample Inference Time (ms): {avg_sample_inference_time}\")\n",
    "print(f\"Batch Inference Time (ms): {avg_batch_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c772fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov12_df = pd.DataFrame({\n",
    "    \"Model\" : [\"YOLOv12-m\"],\n",
    "    \"Model_size (MB)\" : [model_size],\n",
    "    \"Num_Parameters (Million)\": [M_params],\n",
    "    \"Sample MACs (10^9)\" : [M_MACs],\n",
    "    \"Sample FLOPs (10^9)\" : [M_FLOPs],\n",
    "    \"Batch MACs (10^9)\" : [M_MACs_batch],\n",
    "    \"Batch FLOPs (10^9)\" : [M_FLOPs_batch],\n",
    "    \"Sample Latency (ms)\": [avg_sample_inference_time],\n",
    "    \"Batch Latency (ms)\": [avg_batch_inference_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220639b",
   "metadata": {},
   "source": [
    "# 6) Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffab8b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_size (MB)</th>\n",
       "      <th>Num_Parameters (Million)</th>\n",
       "      <th>Sample MACs (10^9)</th>\n",
       "      <th>Sample FLOPs (10^9)</th>\n",
       "      <th>Batch MACs (10^9)</th>\n",
       "      <th>Batch FLOPs (10^9)</th>\n",
       "      <th>Sample Latency (ms)</th>\n",
       "      <th>Batch Latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOLOv8I-m</td>\n",
       "      <td>77.594</td>\n",
       "      <td>20.280884</td>\n",
       "      <td>24.336882</td>\n",
       "      <td>48.673763</td>\n",
       "      <td>389.390104</td>\n",
       "      <td>778.780208</td>\n",
       "      <td>5.876</td>\n",
       "      <td>94.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOLOv8-m</td>\n",
       "      <td>98.938</td>\n",
       "      <td>25.902640</td>\n",
       "      <td>29.745158</td>\n",
       "      <td>59.490317</td>\n",
       "      <td>475.922534</td>\n",
       "      <td>951.845068</td>\n",
       "      <td>6.425</td>\n",
       "      <td>102.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOLOv12-m</td>\n",
       "      <td>77.311</td>\n",
       "      <td>20.201216</td>\n",
       "      <td>25.529722</td>\n",
       "      <td>51.059443</td>\n",
       "      <td>408.475546</td>\n",
       "      <td>816.951092</td>\n",
       "      <td>8.345</td>\n",
       "      <td>133.523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Model_size (MB)  Num_Parameters (Million)  Sample MACs (10^9)  \\\n",
       "0  YOLOv8I-m           77.594                 20.280884           24.336882   \n",
       "1   YOLOv8-m           98.938                 25.902640           29.745158   \n",
       "2  YOLOv12-m           77.311                 20.201216           25.529722   \n",
       "\n",
       "   Sample FLOPs (10^9)  Batch MACs (10^9)  Batch FLOPs (10^9)  \\\n",
       "0            48.673763         389.390104          778.780208   \n",
       "1            59.490317         475.922534          951.845068   \n",
       "2            51.059443         408.475546          816.951092   \n",
       "\n",
       "   Sample Latency (ms)  Batch Latency (ms)  \n",
       "0                5.876              94.009  \n",
       "1                6.425             102.805  \n",
       "2                8.345             133.523  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.concat([model_df, yolov8_df, yolov12_df], ignore_index=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e343c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
